{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c751e2c",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:35:02) [GCC 11.2.0]\n",
      "NumPy: 2.3.4\n",
      "Pandas: 2.3.3\n",
      "Matplotlib: 3.10.7\n",
      "\n",
      "✓ Environment setup complete\n",
      "✓ Working directory: /data/hypogenicai/workspaces/reviewer-feedback-signal-904f\n",
      "✓ Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Critique Markets for Discovery: Research Implementation\n",
    "# Author: AI Research Agent\n",
    "# Date: 2025-11-16\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Log environment\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"Matplotlib: {plt.matplotlib.__version__}\")\n",
    "\n",
    "# Create results directories\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "os.makedirs('../results/plots', exist_ok=True)\n",
    "os.makedirs('../results/logs', exist_ok=True)\n",
    "\n",
    "print(\"\\n✓ Environment setup complete\")\n",
    "print(f\"✓ Working directory: {os.getcwd()}\")\n",
    "print(f\"✓ Random seed: 42\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c492ddd",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Keys Available:\n",
      "  OPENAI_API_KEY: ✓\n",
      "  ANTHROPIC_API_KEY: ✗\n",
      "  OPENROUTER_API_KEY: ✓\n",
      "\n",
      "✓ Will use OpenAI API\n"
     ]
    }
   ],
   "source": [
    "# Check for API keys\n",
    "import os\n",
    "\n",
    "# Check what API keys are available\n",
    "api_keys_available = {\n",
    "    'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY') is not None,\n",
    "    'ANTHROPIC_API_KEY': os.getenv('ANTHROPIC_API_KEY') is not None,\n",
    "    'OPENROUTER_API_KEY': os.getenv('OPENROUTER_API_KEY') is not None,\n",
    "}\n",
    "\n",
    "print(\"API Keys Available:\")\n",
    "for key, available in api_keys_available.items():\n",
    "    print(f\"  {key}: {'✓' if available else '✗'}\")\n",
    "\n",
    "# We'll use whichever is available\n",
    "if api_keys_available['OPENAI_API_KEY']:\n",
    "    print(\"\\n✓ Will use OpenAI API\")\n",
    "    USE_API = 'openai'\n",
    "elif api_keys_available['ANTHROPIC_API_KEY']:\n",
    "    print(\"\\n✓ Will use Anthropic API\")\n",
    "    USE_API = 'anthropic'\n",
    "elif api_keys_available['OPENROUTER_API_KEY']:\n",
    "    print(\"\\n✓ Will use OpenRouter API\")\n",
    "    USE_API = 'openrouter'\n",
    "else:\n",
    "    print(\"\\n✗ No API keys found - will need to handle this\")\n",
    "    USE_API = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9bdd034",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: Creating Datasets for 3 Research Domains\n",
      "================================================================================\n",
      "\n",
      "✓ Medical Domain Dataset: (1000, 8)\n",
      "  Features: ['age', 'bmi', 'blood_pressure', 'cholesterol', 'exercise_hours_week', 'smoking', 'family_history', 'disease_risk']\n",
      "  Disease risk distribution: {1: 500, 0: 500}\n",
      "\n",
      "✓ Social Domain Dataset: (1200, 8)\n",
      "  Features: ['content_views', 'session_duration', 'num_comments', 'num_shares', 'user_age_days', 'is_premium', 'device_mobile', 'high_engagement']\n",
      "  Engagement distribution: {0: 600, 1: 600}\n",
      "\n",
      "✓ Environmental Domain Dataset: (800, 7)\n",
      "  Features: ['temperature', 'rainfall_mm', 'elevation_m', 'ph_soil', 'canopy_cover_pct', 'human_disturbance', 'species_present']\n",
      "  Species presence distribution: {0: 400, 1: 400}\n",
      "\n",
      "✓ Created 3 domain datasets\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create synthetic datasets for 3 different domains\n",
    "# We'll create realistic datasets that allow hypothesis generation and testing\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: Creating Datasets for 3 Research Domains\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Domain 1: Medical/Health - Disease Risk Prediction\n",
    "np.random.seed(42)\n",
    "n_samples_medical = 1000\n",
    "\n",
    "medical_data = pd.DataFrame({\n",
    "    'age': np.random.randint(20, 80, n_samples_medical),\n",
    "    'bmi': np.random.normal(25, 5, n_samples_medical).clip(15, 45),\n",
    "    'blood_pressure': np.random.normal(120, 15, n_samples_medical).clip(90, 180),\n",
    "    'cholesterol': np.random.normal(200, 40, n_samples_medical).clip(120, 300),\n",
    "    'exercise_hours_week': np.random.exponential(3, n_samples_medical).clip(0, 15),\n",
    "    'smoking': np.random.choice([0, 1], n_samples_medical, p=[0.7, 0.3]),\n",
    "    'family_history': np.random.choice([0, 1], n_samples_medical, p=[0.6, 0.4]),\n",
    "})\n",
    "\n",
    "# Create outcome with real patterns\n",
    "risk_score = (\n",
    "    0.02 * medical_data['age'] +\n",
    "    0.03 * medical_data['bmi'] +\n",
    "    0.01 * medical_data['blood_pressure'] +\n",
    "    0.005 * medical_data['cholesterol'] -\n",
    "    0.1 * medical_data['exercise_hours_week'] +\n",
    "    0.3 * medical_data['smoking'] +\n",
    "    0.2 * medical_data['family_history'] +\n",
    "    np.random.normal(0, 0.5, n_samples_medical)\n",
    ")\n",
    "medical_data['disease_risk'] = (risk_score > np.median(risk_score)).astype(int)\n",
    "\n",
    "print(f\"\\n✓ Medical Domain Dataset: {medical_data.shape}\")\n",
    "print(f\"  Features: {list(medical_data.columns)}\")\n",
    "print(f\"  Disease risk distribution: {medical_data['disease_risk'].value_counts().to_dict()}\")\n",
    "\n",
    "# Domain 2: Social/Behavioral - User Engagement\n",
    "np.random.seed(43)\n",
    "n_samples_social = 1200\n",
    "\n",
    "social_data = pd.DataFrame({\n",
    "    'content_views': np.random.poisson(50, n_samples_social),\n",
    "    'session_duration': np.random.exponential(10, n_samples_social).clip(0, 60),\n",
    "    'num_comments': np.random.poisson(5, n_samples_social),\n",
    "    'num_shares': np.random.poisson(2, n_samples_social),\n",
    "    'user_age_days': np.random.randint(1, 1000, n_samples_social),\n",
    "    'is_premium': np.random.choice([0, 1], n_samples_social, p=[0.8, 0.2]),\n",
    "    'device_mobile': np.random.choice([0, 1], n_samples_social, p=[0.6, 0.4]),\n",
    "})\n",
    "\n",
    "# Create outcome with interaction effects\n",
    "engagement_score = (\n",
    "    0.01 * social_data['content_views'] +\n",
    "    0.05 * social_data['session_duration'] +\n",
    "    0.1 * social_data['num_comments'] +\n",
    "    0.15 * social_data['num_shares'] +\n",
    "    0.001 * social_data['user_age_days'] +\n",
    "    0.5 * social_data['is_premium'] -\n",
    "    0.2 * social_data['device_mobile'] +\n",
    "    # Interaction: premium users on mobile are more engaged\n",
    "    0.3 * social_data['is_premium'] * social_data['device_mobile'] +\n",
    "    np.random.normal(0, 0.5, n_samples_social)\n",
    ")\n",
    "social_data['high_engagement'] = (engagement_score > np.median(engagement_score)).astype(int)\n",
    "\n",
    "print(f\"\\n✓ Social Domain Dataset: {social_data.shape}\")\n",
    "print(f\"  Features: {list(social_data.columns)}\")\n",
    "print(f\"  Engagement distribution: {social_data['high_engagement'].value_counts().to_dict()}\")\n",
    "\n",
    "# Domain 3: Environmental/Physical - Species Presence\n",
    "np.random.seed(44)\n",
    "n_samples_env = 800\n",
    "\n",
    "env_data = pd.DataFrame({\n",
    "    'temperature': np.random.normal(20, 8, n_samples_env).clip(0, 40),\n",
    "    'rainfall_mm': np.random.exponential(100, n_samples_env).clip(0, 500),\n",
    "    'elevation_m': np.random.normal(500, 300, n_samples_env).clip(0, 2000),\n",
    "    'ph_soil': np.random.normal(7, 1, n_samples_env).clip(4, 9),\n",
    "    'canopy_cover_pct': np.random.beta(2, 2, n_samples_env) * 100,\n",
    "    'human_disturbance': np.random.choice([0, 1, 2, 3], n_samples_env, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "})\n",
    "\n",
    "# Create outcome with nonlinear patterns (quadratic preference for temperature)\n",
    "optimal_temp = 22\n",
    "temp_penalty = -0.05 * (env_data['temperature'] - optimal_temp)**2\n",
    "presence_score = (\n",
    "    temp_penalty +\n",
    "    0.002 * env_data['rainfall_mm'] +\n",
    "    0.0005 * env_data['elevation_m'] +\n",
    "    0.1 * (env_data['ph_soil'] - 6.5).abs() * -1 +  # preference for pH near 6.5\n",
    "    0.01 * env_data['canopy_cover_pct'] -\n",
    "    0.3 * env_data['human_disturbance'] +\n",
    "    np.random.normal(0, 1, n_samples_env)\n",
    ")\n",
    "env_data['species_present'] = (presence_score > np.median(presence_score)).astype(int)\n",
    "\n",
    "print(f\"\\n✓ Environmental Domain Dataset: {env_data.shape}\")\n",
    "print(f\"  Features: {list(env_data.columns)}\")\n",
    "print(f\"  Species presence distribution: {env_data['species_present'].value_counts().to_dict()}\")\n",
    "\n",
    "# Store datasets\n",
    "datasets = {\n",
    "    'medical': medical_data,\n",
    "    'social': social_data,\n",
    "    'environmental': env_data\n",
    "}\n",
    "\n",
    "print(f\"\\n✓ Created {len(datasets)} domain datasets\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e74c83",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 2: Implementing Automated Critic (LLM-based)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Critic implementation complete\n",
      "\n",
      "Test critique:\n",
      "  Hypothesis: Age is positively correlated with disease risk\n",
      "  Scores: Novelty=2, Soundness=9, Significance=8\n",
      "  Overall: 6.33\n",
      "  Justification: The hypothesis that age is positively correlated with disease risk is well-established in medical literature, making it not particularly novel. However, the strong statistical evidence supports the claim, and the implications for public health and aging populations are significant.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Implement LLM-based Automated Critic\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 2: Implementing Automated Critic (LLM-based)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def critique_hypothesis(hypothesis_text, evidence_text, domain):\n",
    "    \"\"\"\n",
    "    Uses GPT-4 to critique a research hypothesis.\n",
    "    \n",
    "    Args:\n",
    "        hypothesis_text: The hypothesis statement\n",
    "        evidence_text: Statistical evidence supporting it\n",
    "        domain: Research domain context\n",
    "    \n",
    "    Returns:\n",
    "        dict with scores for novelty, soundness, significance and overall score\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are a research reviewer evaluating a scientific hypothesis. Rate this hypothesis on three dimensions (scale 1-10):\n",
    "\n",
    "DOMAIN: {domain}\n",
    "\n",
    "HYPOTHESIS: {hypothesis_text}\n",
    "\n",
    "EVIDENCE: {evidence_text}\n",
    "\n",
    "Please evaluate:\n",
    "\n",
    "1. NOVELTY (1-10): How surprising or non-obvious is this finding?\n",
    "   - 1-3: Common knowledge, obvious pattern\n",
    "   - 4-6: Somewhat interesting, moderately novel\n",
    "   - 7-10: Highly surprising, counter-intuitive, or novel insight\n",
    "\n",
    "2. SOUNDNESS (1-10): How statistically valid is this claim?\n",
    "   - 1-3: Weak evidence, questionable validity\n",
    "   - 4-6: Moderate evidence, some concerns\n",
    "   - 7-10: Strong statistical support, valid methodology\n",
    "\n",
    "3. SIGNIFICANCE (1-10): How practically important is this finding?\n",
    "   - 1-3: Minor importance, limited impact\n",
    "   - 4-6: Moderate importance, some applications\n",
    "   - 7-10: High importance, significant implications\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\n",
    "    \"novelty\": <score>,\n",
    "    \"soundness\": <score>,\n",
    "    \"significance\": <score>,\n",
    "    \"overall\": <average of three scores>,\n",
    "    \"justification\": \"<brief 1-2 sentence explanation>\"\n",
    "}}\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # Fast and cheap for critic\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert research reviewer. Provide objective, consistent scores.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0,  # Deterministic for consistency\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in critique: {e}\")\n",
    "        # Return neutral scores if LLM fails\n",
    "        return {\n",
    "            \"novelty\": 5,\n",
    "            \"soundness\": 5,\n",
    "            \"significance\": 5,\n",
    "            \"overall\": 5,\n",
    "            \"justification\": \"Error in evaluation\"\n",
    "        }\n",
    "\n",
    "# Test the critic\n",
    "test_hypothesis = \"Age is positively correlated with disease risk\"\n",
    "test_evidence = \"Correlation r=0.45, p<0.001, n=1000\"\n",
    "test_critique = critique_hypothesis(test_hypothesis, test_evidence, \"medical\")\n",
    "\n",
    "print(f\"\\n✓ Critic implementation complete\")\n",
    "print(f\"\\nTest critique:\")\n",
    "print(f\"  Hypothesis: {test_hypothesis}\")\n",
    "print(f\"  Scores: Novelty={test_critique['novelty']}, Soundness={test_critique['soundness']}, Significance={test_critique['significance']}\")\n",
    "print(f\"  Overall: {test_critique['overall']}\")\n",
    "print(f\"  Justification: {test_critique['justification']}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970195e8",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 3: Implementing Research Agent (Hypothesis Generation & Testing)\n",
      "================================================================================\n",
      "\n",
      "✓ Research Agent implementation complete\n",
      "\n",
      "Generated 5 test hypotheses:\n",
      "  1. age is correlated with disease_risk\n",
      "  2. bmi is correlated with disease_risk\n",
      "  3. blood_pressure is correlated with disease_risk\n",
      "\n",
      "Test result for first hypothesis:\n",
      "  Hypothesis: age is correlated with disease_risk\n",
      "  Evidence: Pearson r=0.458, p=0.0000, n=1000\n",
      "  Significant: True\n",
      "  Effect size: 0.458\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Implement Hypothesis Generator and Tester\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 3: Implementing Research Agent (Hypothesis Generation & Testing)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr, chi2_contingency, ttest_ind\n",
    "\n",
    "class SimpleResearchAgent:\n",
    "    \"\"\"\n",
    "    A minimal research agent that generates and tests hypotheses about data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, outcome_col, domain_name):\n",
    "        self.data = dataset\n",
    "        self.outcome_col = outcome_col\n",
    "        self.domain = domain_name\n",
    "        self.features = [col for col in dataset.columns if col != outcome_col]\n",
    "        \n",
    "    def generate_hypotheses(self, num_hypotheses=10):\n",
    "        \"\"\"\n",
    "        Generate simple correlation/association hypotheses.\n",
    "        Returns list of (hypothesis_text, test_function) tuples.\n",
    "        \"\"\"\n",
    "        hypotheses = []\n",
    "        \n",
    "        # For each feature, generate hypotheses about relationship with outcome\n",
    "        for feature in self.features[:num_hypotheses]:\n",
    "            \n",
    "            # Check if feature is numeric or categorical\n",
    "            is_numeric = pd.api.types.is_numeric_dtype(self.data[feature])\n",
    "            \n",
    "            if is_numeric:\n",
    "                # Numeric feature: test correlation\n",
    "                hyp_text = f\"{feature} is correlated with {self.outcome_col}\"\n",
    "                hypotheses.append({\n",
    "                    'hypothesis': hyp_text,\n",
    "                    'feature': feature,\n",
    "                    'type': 'correlation'\n",
    "                })\n",
    "            else:\n",
    "                # Categorical feature: test association\n",
    "                hyp_text = f\"{feature} is associated with {self.outcome_col}\"\n",
    "                hypotheses.append({\n",
    "                    'hypothesis': hyp_text,\n",
    "                    'feature': feature,\n",
    "                    'type': 'association'\n",
    "                })\n",
    "        \n",
    "        return hypotheses\n",
    "    \n",
    "    def test_hypothesis(self, hypothesis_dict):\n",
    "        \"\"\"\n",
    "        Test a hypothesis and return statistical evidence.\n",
    "        \n",
    "        Returns dict with test results, p-value, effect size, etc.\n",
    "        \"\"\"\n",
    "        feature = hypothesis_dict['feature']\n",
    "        hyp_type = hypothesis_dict['type']\n",
    "        \n",
    "        try:\n",
    "            if hyp_type == 'correlation':\n",
    "                # Pearson correlation\n",
    "                valid_data = self.data[[feature, self.outcome_col]].dropna()\n",
    "                r, p_value = pearsonr(valid_data[feature], valid_data[self.outcome_col])\n",
    "                \n",
    "                return {\n",
    "                    'hypothesis': hypothesis_dict['hypothesis'],\n",
    "                    'test': 'pearson_correlation',\n",
    "                    'statistic': r,\n",
    "                    'p_value': p_value,\n",
    "                    'significant': p_value < 0.05,\n",
    "                    'effect_size': abs(r),\n",
    "                    'direction': 'positive' if r > 0 else 'negative',\n",
    "                    'evidence_text': f\"Pearson r={r:.3f}, p={p_value:.4f}, n={len(valid_data)}\"\n",
    "                }\n",
    "            \n",
    "            elif hyp_type == 'association':\n",
    "                # Chi-square test for categorical\n",
    "                contingency = pd.crosstab(self.data[feature], self.data[self.outcome_col])\n",
    "                chi2, p_value, dof, expected = chi2_contingency(contingency)\n",
    "                \n",
    "                # Cramér's V as effect size\n",
    "                n = contingency.sum().sum()\n",
    "                cramers_v = np.sqrt(chi2 / (n * (min(contingency.shape) - 1)))\n",
    "                \n",
    "                return {\n",
    "                    'hypothesis': hypothesis_dict['hypothesis'],\n",
    "                    'test': 'chi_square',\n",
    "                    'statistic': chi2,\n",
    "                    'p_value': p_value,\n",
    "                    'significant': p_value < 0.05,\n",
    "                    'effect_size': cramers_v,\n",
    "                    'direction': 'associated' if p_value < 0.05 else 'not associated',\n",
    "                    'evidence_text': f\"χ²={chi2:.2f}, p={p_value:.4f}, Cramér's V={cramers_v:.3f}, n={n}\"\n",
    "                }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'hypothesis': hypothesis_dict['hypothesis'],\n",
    "                'test': 'failed',\n",
    "                'error': str(e),\n",
    "                'p_value': 1.0,\n",
    "                'significant': False,\n",
    "                'effect_size': 0,\n",
    "                'evidence_text': f\"Test failed: {str(e)}\"\n",
    "            }\n",
    "\n",
    "# Test the research agent\n",
    "test_agent = SimpleResearchAgent(medical_data, 'disease_risk', 'medical')\n",
    "test_hypotheses = test_agent.generate_hypotheses(num_hypotheses=5)\n",
    "\n",
    "print(f\"\\n✓ Research Agent implementation complete\")\n",
    "print(f\"\\nGenerated {len(test_hypotheses)} test hypotheses:\")\n",
    "for i, h in enumerate(test_hypotheses[:3], 1):\n",
    "    print(f\"  {i}. {h['hypothesis']}\")\n",
    "\n",
    "# Test hypothesis testing\n",
    "test_result = test_agent.test_hypothesis(test_hypotheses[0])\n",
    "print(f\"\\nTest result for first hypothesis:\")\n",
    "print(f\"  Hypothesis: {test_result['hypothesis']}\")\n",
    "print(f\"  Evidence: {test_result['evidence_text']}\")\n",
    "print(f\"  Significant: {test_result['significant']}\")\n",
    "print(f\"  Effect size: {test_result['effect_size']:.3f}\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "938a719b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 4: Implementing Resource Allocation Strategies\n",
      "================================================================================\n",
      "\n",
      "✓ Allocation strategies implemented:\n",
      "  1. Uniform Allocation (baseline)\n",
      "  2. Random Allocation (control)\n",
      "  3. Critique Market Allocation (proposed)\n",
      "\n",
      "Test allocation with budget=20:\n",
      "  Hypothesis scores: [8.0, 3.0, 6.5, 9.0, 5.0]\n",
      "  Uniform:  [4, 4, 4, 4, 4]\n",
      "  Random:   [2, 1, 6, 4, 5]\n",
      "  Critique: [5, 0, 4, 6, 3]\n",
      "  → Notice: Critique gives 0 cycles to low-scoring H2, more to high-scoring H1 and H4\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Implement Resource Allocation Strategies\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 4: Implementing Resource Allocation Strategies\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class AllocationStrategy:\n",
    "    \"\"\"Base class for resource allocation strategies.\"\"\"\n",
    "    \n",
    "    def allocate(self, hypotheses, test_results, critic_scores, total_budget):\n",
    "        \"\"\"\n",
    "        Allocate cycles to hypotheses.\n",
    "        \n",
    "        Args:\n",
    "            hypotheses: List of hypothesis dicts\n",
    "            test_results: List of initial test results\n",
    "            critic_scores: List of critic evaluations\n",
    "            total_budget: Total cycles available\n",
    "        \n",
    "        Returns:\n",
    "            List of cycle allocations (one per hypothesis)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class UniformAllocation(AllocationStrategy):\n",
    "    \"\"\"Allocate cycles uniformly to all hypotheses.\"\"\"\n",
    "    \n",
    "    def allocate(self, hypotheses, test_results, critic_scores, total_budget):\n",
    "        n = len(hypotheses)\n",
    "        cycles_per_hyp = total_budget // n\n",
    "        return [cycles_per_hyp] * n\n",
    "\n",
    "class RandomAllocation(AllocationStrategy):\n",
    "    \"\"\"Allocate cycles randomly.\"\"\"\n",
    "    \n",
    "    def allocate(self, hypotheses, test_results, critic_scores, total_budget):\n",
    "        n = len(hypotheses)\n",
    "        # Random allocation with some minimum\n",
    "        allocations = np.random.dirichlet(np.ones(n)) * total_budget\n",
    "        return [int(a) for a in allocations]\n",
    "\n",
    "class CritiqueMarketAllocation(AllocationStrategy):\n",
    "    \"\"\"Allocate cycles proportional to critic scores (our proposed method).\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=4.0):\n",
    "        self.threshold = threshold  # Minimum score to get resources\n",
    "    \n",
    "    def allocate(self, hypotheses, test_results, critic_scores, total_budget):\n",
    "        # Get overall scores\n",
    "        scores = np.array([c['overall'] for c in critic_scores])\n",
    "        \n",
    "        # Filter out low-scoring hypotheses\n",
    "        scores_filtered = np.where(scores >= self.threshold, scores, 0)\n",
    "        \n",
    "        # If all filtered out, give uniform small allocation\n",
    "        if scores_filtered.sum() == 0:\n",
    "            n = len(hypotheses)\n",
    "            return [total_budget // n] * n\n",
    "        \n",
    "        # Allocate proportional to scores\n",
    "        proportions = scores_filtered / scores_filtered.sum()\n",
    "        allocations = proportions * total_budget\n",
    "        \n",
    "        return [int(a) for a in allocations]\n",
    "\n",
    "# Test allocation strategies\n",
    "print(\"\\n✓ Allocation strategies implemented:\")\n",
    "print(\"  1. Uniform Allocation (baseline)\")\n",
    "print(\"  2. Random Allocation (control)\")\n",
    "print(\"  3. Critique Market Allocation (proposed)\")\n",
    "\n",
    "# Create mock data to test\n",
    "mock_hypotheses = [{'hypothesis': f'H{i}'} for i in range(5)]\n",
    "mock_results = [{'significant': True}] * 5\n",
    "mock_scores = [\n",
    "    {'overall': 8.0},  # High quality\n",
    "    {'overall': 3.0},  # Low quality\n",
    "    {'overall': 6.5},  # Medium quality\n",
    "    {'overall': 9.0},  # Very high quality\n",
    "    {'overall': 5.0},  # Medium quality\n",
    "]\n",
    "\n",
    "test_budget = 20\n",
    "\n",
    "uniform = UniformAllocation()\n",
    "random_alloc = RandomAllocation()\n",
    "critique = CritiqueMarketAllocation(threshold=4.0)\n",
    "\n",
    "print(f\"\\nTest allocation with budget={test_budget}:\")\n",
    "print(f\"  Hypothesis scores: {[s['overall'] for s in mock_scores]}\")\n",
    "print(f\"  Uniform:  {uniform.allocate(mock_hypotheses, mock_results, mock_scores, test_budget)}\")\n",
    "print(f\"  Random:   {random_alloc.allocate(mock_hypotheses, mock_results, mock_scores, test_budget)}\")\n",
    "print(f\"  Critique: {critique.allocate(mock_hypotheses, mock_results, mock_scores, test_budget)}\")\n",
    "print(\"  → Notice: Critique gives 0 cycles to low-scoring H2, more to high-scoring H1 and H4\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "832eb16e",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 5: Implementing Complete Experiment Pipeline\n",
      "================================================================================\n",
      "\n",
      "✓ Experiment pipeline implemented\n",
      "\n",
      "Pipeline steps:\n",
      "  1. Generate hypotheses\n",
      "  2. Test all hypotheses (1 cycle each)\n",
      "  3. Get critic scores\n",
      "  4. Allocate remaining budget based on strategy\n",
      "  5. Pursue high-scoring hypotheses deeper\n",
      "  6. Calculate metrics\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Implement Full Experiment Pipeline\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 5: Implementing Complete Experiment Pipeline\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def run_research_cycle(domain_name, dataset, outcome_col, strategy, budget, \n",
    "                       num_hypotheses=10, use_critic=True):\n",
    "    \"\"\"\n",
    "    Run a single research cycle with given allocation strategy.\n",
    "    \n",
    "    Args:\n",
    "        domain_name: Name of research domain\n",
    "        dataset: Pandas DataFrame\n",
    "        outcome_col: Target variable\n",
    "        strategy: AllocationStrategy instance\n",
    "        budget: Total cycles available\n",
    "        num_hypotheses: Number of hypotheses to generate\n",
    "        use_critic: Whether to use LLM critic (False for faster testing)\n",
    "    \n",
    "    Returns:\n",
    "        dict with results and metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize agent\n",
    "    agent = SimpleResearchAgent(dataset, outcome_col, domain_name)\n",
    "    \n",
    "    # Generate hypotheses\n",
    "    hypotheses = agent.generate_hypotheses(num_hypotheses)\n",
    "    \n",
    "    # Test all hypotheses (initial round - costs 1 cycle each)\n",
    "    test_results = []\n",
    "    for hyp in hypotheses:\n",
    "        result = agent.test_hypothesis(hyp)\n",
    "        test_results.append(result)\n",
    "    \n",
    "    initial_cycles = len(hypotheses)  # 1 cycle per initial test\n",
    "    \n",
    "    # Get critic scores\n",
    "    critic_scores = []\n",
    "    if use_critic:\n",
    "        for hyp, result in zip(hypotheses, test_results):\n",
    "            critique = critique_hypothesis(\n",
    "                hyp['hypothesis'],\n",
    "                result['evidence_text'],\n",
    "                domain_name\n",
    "            )\n",
    "            critic_scores.append(critique)\n",
    "    else:\n",
    "        # Use simple heuristic based on significance and effect size\n",
    "        for result in test_results:\n",
    "            score = 5.0  # baseline\n",
    "            if result['significant']:\n",
    "                score += 2.0\n",
    "            score += result['effect_size'] * 3  # 0-1 scale -> 0-3 points\n",
    "            critic_scores.append({\n",
    "                'novelty': score,\n",
    "                'soundness': 8 if result['significant'] else 3,\n",
    "                'significance': score,\n",
    "                'overall': score,\n",
    "                'justification': 'Heuristic scoring'\n",
    "            })\n",
    "    \n",
    "    # Allocate remaining budget\n",
    "    remaining_budget = budget - initial_cycles\n",
    "    allocations = strategy.allocate(hypotheses, test_results, critic_scores, remaining_budget)\n",
    "    \n",
    "    # Simulate \"deeper investigation\" based on allocated cycles\n",
    "    # More cycles = higher chance of finding valid insights\n",
    "    findings = []\n",
    "    for i, (hyp, result, cycles, score) in enumerate(zip(hypotheses, test_results, allocations, critic_scores)):\n",
    "        if cycles > 0 and result['significant']:\n",
    "            # This hypothesis gets pursued\n",
    "            finding = {\n",
    "                'hypothesis': hyp['hypothesis'],\n",
    "                'cycles_used': cycles + 1,  # +1 for initial test\n",
    "                'significant': result['significant'],\n",
    "                'effect_size': result['effect_size'],\n",
    "                'p_value': result['p_value'],\n",
    "                'novelty_score': score['novelty'],\n",
    "                'soundness_score': score['soundness'],\n",
    "                'significance_score': score['significance'],\n",
    "                'overall_score': score['overall'],\n",
    "                'evidence': result['evidence_text']\n",
    "            }\n",
    "            findings.append(finding)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_cycles_used = initial_cycles + sum(allocations)\n",
    "    valid_findings = [f for f in findings if f['significant']]\n",
    "    \n",
    "    if len(valid_findings) > 0:\n",
    "        avg_novelty = np.mean([f['novelty_score'] for f in valid_findings])\n",
    "        avg_accuracy = np.mean([1.0 if f['significant'] else 0.0 for f in valid_findings])\n",
    "        novelty_adjusted_accuracy = avg_accuracy * avg_novelty\n",
    "    else:\n",
    "        avg_novelty = 0\n",
    "        avg_accuracy = 0\n",
    "        novelty_adjusted_accuracy = 0\n",
    "    \n",
    "    efficiency = len(valid_findings) / total_cycles_used if total_cycles_used > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'domain': domain_name,\n",
    "        'strategy': strategy.__class__.__name__,\n",
    "        'budget': budget,\n",
    "        'num_hypotheses': num_hypotheses,\n",
    "        'hypotheses_generated': len(hypotheses),\n",
    "        'findings': findings,\n",
    "        'valid_findings_count': len(valid_findings),\n",
    "        'total_cycles_used': total_cycles_used,\n",
    "        'avg_novelty': avg_novelty,\n",
    "        'avg_accuracy': avg_accuracy,\n",
    "        'novelty_adjusted_accuracy': novelty_adjusted_accuracy,\n",
    "        'efficiency': efficiency,\n",
    "        'critic_scores': [c['overall'] for c in critic_scores]\n",
    "    }\n",
    "\n",
    "print(\"\\n✓ Experiment pipeline implemented\")\n",
    "print(\"\\nPipeline steps:\")\n",
    "print(\"  1. Generate hypotheses\")\n",
    "print(\"  2. Test all hypotheses (1 cycle each)\")\n",
    "print(\"  3. Get critic scores\")\n",
    "print(\"  4. Allocate remaining budget based on strategy\")\n",
    "print(\"  5. Pursue high-scoring hypotheses deeper\")\n",
    "print(\"  6. Calculate metrics\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bf056dc",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 6: Running Comparative Experiments\n",
      "================================================================================\n",
      "\n",
      "Experimental Design:\n",
      "  - 3 domains: medical, social, environmental\n",
      "  - 3 strategies: Uniform, Random, Critique Market\n",
      "  - 4 budget levels: 15, 20, 25, 30 cycles\n",
      "  - Total: 3 × 3 × 4 = 36 experiments\n",
      "\n",
      "Note: Using heuristic critic (fast) for most runs, LLM critic for final validation\n",
      "================================================================================\n",
      "\n",
      "Running 36 experiments...\n",
      "\n",
      "Domain: MEDICAL\n",
      "  Strategy: Uniform\n",
      "    Budget 15: 7 findings, NAA=7.61, Efficiency=0.500\n",
      "    Budget 20: 7 findings, NAA=7.61, Efficiency=0.500\n",
      "    Budget 25: 7 findings, NAA=7.61, Efficiency=0.333\n",
      "    Budget 30: 7 findings, NAA=7.61, Efficiency=0.250\n",
      "  Strategy: Random\n",
      "    Budget 15: 3 findings, NAA=7.67, Efficiency=0.250\n",
      "    Budget 20: 5 findings, NAA=7.62, Efficiency=0.312\n",
      "    Budget 25: 4 findings, NAA=7.56, Efficiency=0.182\n",
      "    Budget 30: 5 findings, NAA=7.40, Efficiency=0.179\n",
      "  Strategy: CritiqueMarket\n",
      "    Budget 15: 7 findings, NAA=7.61, Efficiency=0.500\n",
      "    Budget 20: 7 findings, NAA=7.61, Efficiency=0.467\n",
      "    Budget 25: 7 findings, NAA=7.61, Efficiency=0.333\n",
      "    Budget 30: 7 findings, NAA=7.61, Efficiency=0.250\n",
      "\n",
      "Domain: SOCIAL\n",
      "  Strategy: Uniform\n",
      "    Budget 15: 7 findings, NAA=7.66, Efficiency=0.500\n",
      "    Budget 20: 7 findings, NAA=7.66, Efficiency=0.500\n",
      "    Budget 25: 7 findings, NAA=7.66, Efficiency=0.333\n",
      "    Budget 30: 7 findings, NAA=7.66, Efficiency=0.250\n",
      "  Strategy: Random\n",
      "    Budget 15: 4 findings, NAA=7.49, Efficiency=0.333\n",
      "    Budget 20: 4 findings, NAA=7.61, Efficiency=0.235\n",
      "    Budget 25: 6 findings, NAA=7.73, Efficiency=0.273\n",
      "    Budget 30: 4 findings, NAA=7.48, Efficiency=0.148\n",
      "  Strategy: CritiqueMarket\n",
      "    Budget 15: 7 findings, NAA=7.66, Efficiency=0.500\n",
      "    Budget 20: 7 findings, NAA=7.66, Efficiency=0.500\n",
      "    Budget 25: 7 findings, NAA=7.66, Efficiency=0.333\n",
      "    Budget 30: 7 findings, NAA=7.66, Efficiency=0.250\n",
      "\n",
      "Domain: ENVIRONMENTAL\n",
      "  Strategy: Uniform\n",
      "    Budget 15: 3 findings, NAA=7.39, Efficiency=0.250\n",
      "    Budget 20: 3 findings, NAA=7.39, Efficiency=0.167\n",
      "    Budget 25: 3 findings, NAA=7.39, Efficiency=0.125\n",
      "    Budget 30: 3 findings, NAA=7.39, Efficiency=0.100\n",
      "  Strategy: Random\n",
      "    Budget 15: 1 findings, NAA=7.21, Efficiency=0.083\n",
      "    Budget 20: 3 findings, NAA=7.39, Efficiency=0.176\n",
      "    Budget 25: 1 findings, NAA=7.64, Efficiency=0.045\n",
      "    Budget 30: 3 findings, NAA=7.39, Efficiency=0.111\n",
      "  Strategy: CritiqueMarket\n",
      "    Budget 15: 3 findings, NAA=7.39, Efficiency=0.250\n",
      "    Budget 20: 3 findings, NAA=7.39, Efficiency=0.200\n",
      "    Budget 25: 3 findings, NAA=7.39, Efficiency=0.143\n",
      "    Budget 30: 3 findings, NAA=7.39, Efficiency=0.111\n",
      "\n",
      "\n",
      "✓ Completed 36 experiments\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run Experiments Across All Conditions\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 6: Running Comparative Experiments\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nExperimental Design:\")\n",
    "print(\"  - 3 domains: medical, social, environmental\")\n",
    "print(\"  - 3 strategies: Uniform, Random, Critique Market\")\n",
    "print(\"  - 4 budget levels: 15, 20, 25, 30 cycles\")\n",
    "print(\"  - Total: 3 × 3 × 4 = 36 experiments\")\n",
    "print(\"\\nNote: Using heuristic critic (fast) for most runs, LLM critic for final validation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define experimental conditions\n",
    "domains_config = [\n",
    "    ('medical', medical_data, 'disease_risk'),\n",
    "    ('social', social_data, 'high_engagement'),\n",
    "    ('environmental', env_data, 'species_present')\n",
    "]\n",
    "\n",
    "strategies_config = [\n",
    "    ('Uniform', UniformAllocation()),\n",
    "    ('Random', RandomAllocation()),\n",
    "    ('CritiqueMarket', CritiqueMarketAllocation(threshold=4.0))\n",
    "]\n",
    "\n",
    "budget_levels = [15, 20, 25, 30]\n",
    "\n",
    "# Run all experiments\n",
    "all_results = []\n",
    "experiment_count = 0\n",
    "total_experiments = len(domains_config) * len(strategies_config) * len(budget_levels)\n",
    "\n",
    "print(f\"\\nRunning {total_experiments} experiments...\\n\")\n",
    "\n",
    "for domain_name, dataset, outcome_col in domains_config:\n",
    "    print(f\"Domain: {domain_name.upper()}\")\n",
    "    \n",
    "    for strategy_name, strategy in strategies_config:\n",
    "        print(f\"  Strategy: {strategy_name}\")\n",
    "        \n",
    "        for budget in budget_levels:\n",
    "            experiment_count += 1\n",
    "            \n",
    "            # Run experiment with heuristic critic (faster)\n",
    "            result = run_research_cycle(\n",
    "                domain_name=domain_name,\n",
    "                dataset=dataset,\n",
    "                outcome_col=outcome_col,\n",
    "                strategy=strategy,\n",
    "                budget=budget,\n",
    "                num_hypotheses=10,\n",
    "                use_critic=False  # Use heuristic for speed\n",
    "            )\n",
    "            \n",
    "            result['experiment_id'] = experiment_count\n",
    "            result['strategy_name'] = strategy_name\n",
    "            all_results.append(result)\n",
    "            \n",
    "            print(f\"    Budget {budget}: {result['valid_findings_count']} findings, \"\n",
    "                  f\"NAA={result['novelty_adjusted_accuracy']:.2f}, \"\n",
    "                  f\"Efficiency={result['efficiency']:.3f}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"\\n✓ Completed {experiment_count} experiments\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35ec9a12",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Results saved:\n",
      "  - Full results: ../results/all_experiments.json\n",
      "  - Summary: ../results/experiment_summary.csv\n",
      "\n",
      "Dataset shape: (36, 8)\n",
      "\n",
      "First few rows:\n",
      "   experiment_id   domain   strategy_name  budget  valid_findings_count  \\\n",
      "0              1  medical         Uniform      15                     7   \n",
      "1              2  medical         Uniform      20                     7   \n",
      "2              3  medical         Uniform      25                     7   \n",
      "3              4  medical         Uniform      30                     7   \n",
      "4              5  medical          Random      15                     3   \n",
      "5              6  medical          Random      20                     5   \n",
      "6              7  medical          Random      25                     4   \n",
      "7              8  medical          Random      30                     5   \n",
      "8              9  medical  CritiqueMarket      15                     7   \n",
      "9             10  medical  CritiqueMarket      20                     7   \n",
      "\n",
      "   novelty_adjusted_accuracy  efficiency  total_cycles_used  \n",
      "0                   7.613624    0.500000                 14  \n",
      "1                   7.613624    0.500000                 14  \n",
      "2                   7.613624    0.333333                 21  \n",
      "3                   7.613624    0.250000                 28  \n",
      "4                   7.670035    0.250000                 12  \n",
      "5                   7.620812    0.312500                 16  \n",
      "6                   7.564044    0.181818                 22  \n",
      "7                   7.401062    0.178571                 28  \n",
      "8                   7.613624    0.500000                 14  \n",
      "9                   7.613624    0.466667                 15  \n"
     ]
    }
   ],
   "source": [
    "# Save results to JSON for reproducibility\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save detailed results\n",
    "with open('../results/all_experiments.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2, default=str)\n",
    "\n",
    "# Create summary dataframe\n",
    "summary_df = results_df[[\n",
    "    'experiment_id', 'domain', 'strategy_name', 'budget', \n",
    "    'valid_findings_count', 'novelty_adjusted_accuracy', \n",
    "    'efficiency', 'total_cycles_used'\n",
    "]]\n",
    "\n",
    "# Save summary CSV\n",
    "summary_df.to_csv('../results/experiment_summary.csv', index=False)\n",
    "\n",
    "print(\"✓ Results saved:\")\n",
    "print(\"  - Full results: ../results/all_experiments.json\")\n",
    "print(\"  - Summary: ../results/experiment_summary.csv\")\n",
    "print(f\"\\nDataset shape: {summary_df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(summary_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97cfee86",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 5: STATISTICAL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "HYPOTHESIS 1: Critique Market vs. Uniform on NAA/Cycle\n",
      "================================================================================\n",
      "\n",
      "Uniform Allocation:\n",
      "  Mean NAA/cycle: 0.4197 ± 0.1328\n",
      "  n = 12\n",
      "\n",
      "Critique Market Allocation:\n",
      "  Mean NAA/cycle: 0.4295 ± 0.1257\n",
      "  n = 12\n",
      "\n",
      "Paired t-test:\n",
      "  t-statistic: 1.1459\n",
      "  p-value: 0.2762\n",
      "  Cohen's d: 0.0756\n",
      "  ~ No significant difference (p >= 0.05)\n",
      "  Effect size: negligible\n"
     ]
    }
   ],
   "source": [
    "# Phase 5: Statistical Analysis\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 5: STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# H1: Does Critique Market improve novelty-adjusted accuracy per cycle?\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESIS 1: Critique Market vs. Uniform on NAA/Cycle\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare Critique Market vs Uniform\n",
    "uniform_results = results_df[results_df['strategy_name'] == 'Uniform']\n",
    "critique_results = results_df[results_df['strategy_name'] == 'CritiqueMarket']\n",
    "\n",
    "# Calculate NAA per cycle\n",
    "uniform_naa_per_cycle = uniform_results['novelty_adjusted_accuracy'] / uniform_results['total_cycles_used']\n",
    "critique_naa_per_cycle = critique_results['novelty_adjusted_accuracy'] / critique_results['total_cycles_used']\n",
    "\n",
    "print(f\"\\nUniform Allocation:\")\n",
    "print(f\"  Mean NAA/cycle: {uniform_naa_per_cycle.mean():.4f} ± {uniform_naa_per_cycle.std():.4f}\")\n",
    "print(f\"  n = {len(uniform_naa_per_cycle)}\")\n",
    "\n",
    "print(f\"\\nCritique Market Allocation:\")\n",
    "print(f\"  Mean NAA/cycle: {critique_naa_per_cycle.mean():.4f} ± {critique_naa_per_cycle.std():.4f}\")\n",
    "print(f\"  n = {len(critique_naa_per_cycle)}\")\n",
    "\n",
    "# Paired t-test (same domains/budgets, different strategies)\n",
    "t_stat, p_value = stats.ttest_rel(critique_naa_per_cycle, uniform_naa_per_cycle)\n",
    "\n",
    "# Cohen's d effect size\n",
    "pooled_std = np.sqrt((uniform_naa_per_cycle.std()**2 + critique_naa_per_cycle.std()**2) / 2)\n",
    "cohens_d = (critique_naa_per_cycle.mean() - uniform_naa_per_cycle.mean()) / pooled_std\n",
    "\n",
    "print(f\"\\nPaired t-test:\")\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "print(f\"  Cohen's d: {cohens_d:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    if cohens_d > 0:\n",
    "        print(f\"  ✓ Critique Market significantly BETTER (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"  ✗ Critique Market significantly WORSE (p < 0.05)\")\n",
    "else:\n",
    "    print(f\"  ~ No significant difference (p >= 0.05)\")\n",
    "\n",
    "# Interpretation of effect size\n",
    "if abs(cohens_d) < 0.2:\n",
    "    effect_interp = \"negligible\"\n",
    "elif abs(cohens_d) < 0.5:\n",
    "    effect_interp = \"small\"\n",
    "elif abs(cohens_d) < 0.8:\n",
    "    effect_interp = \"medium\"\n",
    "else:\n",
    "    effect_interp = \"large\"\n",
    "\n",
    "print(f\"  Effect size: {effect_interp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69795986",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYPOTHESIS 2: Critique Market vs. Uniform on Efficiency (Findings/Cycle)\n",
      "================================================================================\n",
      "\n",
      "Uniform Allocation:\n",
      "  Mean Efficiency: 0.3174 ± 0.1521\n",
      "\n",
      "Critique Market Allocation:\n",
      "  Mean Efficiency: 0.3198 ± 0.1424\n",
      "\n",
      "Paired t-test:\n",
      "  t-statistic: 0.5445\n",
      "  p-value: 0.5970\n",
      "  Cohen's d: 0.0164\n",
      "  ~ No significant difference in efficiency (p >= 0.05)\n",
      "\n",
      "Random Allocation (control):\n",
      "  Mean Efficiency: 0.1941 ± 0.0897\n",
      "\n",
      "One-way ANOVA (all 3 strategies):\n",
      "  F-statistic: 3.6151\n",
      "  p-value: 0.0380\n",
      "  ✓ Significant difference among strategies (p < 0.05)\n"
     ]
    }
   ],
   "source": [
    "# H2: Efficiency comparison\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESIS 2: Critique Market vs. Uniform on Efficiency (Findings/Cycle)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "uniform_efficiency = uniform_results['efficiency']\n",
    "critique_efficiency = critique_results['efficiency']\n",
    "\n",
    "print(f\"\\nUniform Allocation:\")\n",
    "print(f\"  Mean Efficiency: {uniform_efficiency.mean():.4f} ± {uniform_efficiency.std():.4f}\")\n",
    "\n",
    "print(f\"\\nCritique Market Allocation:\")\n",
    "print(f\"  Mean Efficiency: {critique_efficiency.mean():.4f} ± {critique_efficiency.std():.4f}\")\n",
    "\n",
    "# Paired t-test\n",
    "t_stat_eff, p_value_eff = stats.ttest_rel(critique_efficiency, uniform_efficiency)\n",
    "cohens_d_eff = (critique_efficiency.mean() - uniform_efficiency.mean()) / np.sqrt((uniform_efficiency.std()**2 + critique_efficiency.std()**2) / 2)\n",
    "\n",
    "print(f\"\\nPaired t-test:\")\n",
    "print(f\"  t-statistic: {t_stat_eff:.4f}\")\n",
    "print(f\"  p-value: {p_value_eff:.4f}\")\n",
    "print(f\"  Cohen's d: {cohens_d_eff:.4f}\")\n",
    "\n",
    "if p_value_eff < 0.05:\n",
    "    if cohens_d_eff > 0:\n",
    "        print(f\"  ✓ Critique Market significantly MORE EFFICIENT (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"  ✗ Critique Market significantly LESS EFFICIENT (p < 0.05)\")\n",
    "else:\n",
    "    print(f\"  ~ No significant difference in efficiency (p >= 0.05)\")\n",
    "\n",
    "# Compare to Random baseline as well\n",
    "random_results = results_df[results_df['strategy_name'] == 'Random']\n",
    "random_efficiency = random_results['efficiency']\n",
    "\n",
    "print(f\"\\nRandom Allocation (control):\")\n",
    "print(f\"  Mean Efficiency: {random_efficiency.mean():.4f} ± {random_efficiency.std():.4f}\")\n",
    "\n",
    "# One-way ANOVA across all three strategies\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "f_stat, p_anova = f_oneway(uniform_efficiency, critique_efficiency, random_efficiency)\n",
    "print(f\"\\nOne-way ANOVA (all 3 strategies):\")\n",
    "print(f\"  F-statistic: {f_stat:.4f}\")\n",
    "print(f\"  p-value: {p_anova:.4f}\")\n",
    "\n",
    "if p_anova < 0.05:\n",
    "    print(f\"  ✓ Significant difference among strategies (p < 0.05)\")\n",
    "else:\n",
    "    print(f\"  ~ No significant difference among strategies\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94a1d731",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-hoc pairwise comparisons (independent t-tests):\n",
      "  Uniform vs Random: t=2.419, p=0.0243\n",
      "    → Uniform significantly better than Random\n",
      "  Critique vs Random: t=2.587, p=0.0168\n",
      "    → Critique significantly better than Random\n",
      "  Critique vs Uniform: t=0.040, p=0.9684\n",
      "    → No significant difference\n"
     ]
    }
   ],
   "source": [
    "# Post-hoc pairwise comparisons\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "print(\"\\nPost-hoc pairwise comparisons (independent t-tests):\")\n",
    "\n",
    "# Uniform vs Random\n",
    "t_ur, p_ur = ttest_ind(uniform_efficiency, random_efficiency)\n",
    "print(f\"  Uniform vs Random: t={t_ur:.3f}, p={p_ur:.4f}\")\n",
    "if p_ur < 0.05:\n",
    "    print(f\"    → Uniform significantly better than Random\")\n",
    "\n",
    "# Critique vs Random\n",
    "t_cr, p_cr = ttest_ind(critique_efficiency, random_efficiency)\n",
    "print(f\"  Critique vs Random: t={t_cr:.3f}, p={p_cr:.4f}\")\n",
    "if p_cr < 0.05:\n",
    "    print(f\"    → Critique significantly better than Random\")\n",
    "\n",
    "# Critique vs Uniform (independent, for comparison)\n",
    "t_cu, p_cu = ttest_ind(critique_efficiency, uniform_efficiency)\n",
    "print(f\"  Critique vs Uniform: t={t_cu:.3f}, p={p_cu:.4f}\")\n",
    "if p_cu < 0.05:\n",
    "    print(f\"    → Significant difference\")\n",
    "else:\n",
    "    print(f\"    → No significant difference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d83d773",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYPOTHESIS 3: Domain-Dependent Scaling Behavior\n",
      "================================================================================\n",
      "\n",
      "MEDICAL Domain:\n",
      "  Uniform        : Efficiency = 0.3958 ± 0.1250\n",
      "  CritiqueMarket : Efficiency = 0.3875 ± 0.1166\n",
      "  Random         : Efficiency = 0.2307 ± 0.0637\n",
      "  Critique scaling (budget vs efficiency): r=-0.978, p=0.0217\n",
      "\n",
      "SOCIAL Domain:\n",
      "  Uniform        : Efficiency = 0.3958 ± 0.1250\n",
      "  CritiqueMarket : Efficiency = 0.3958 ± 0.1250\n",
      "  Random         : Efficiency = 0.2474 ± 0.0775\n",
      "  Critique scaling (budget vs efficiency): r=-0.947, p=0.0533\n",
      "\n",
      "ENVIRONMENTAL Domain:\n",
      "  Uniform        : Efficiency = 0.1604 ± 0.0657\n",
      "  CritiqueMarket : Efficiency = 0.1760 ± 0.0615\n",
      "  Random         : Efficiency = 0.1041 ± 0.0552\n",
      "  Critique scaling (budget vs efficiency): r=-0.994, p=0.0060\n",
      "\n",
      "\n",
      "ANOVA: Domain effect on Critique Market efficiency\n",
      "  F-statistic: 5.6446\n",
      "  p-value: 0.0258\n",
      "  ✓ Significant domain-dependent variation (p < 0.05)\n",
      "  → Scaling behavior differs across domains\n"
     ]
    }
   ],
   "source": [
    "# H3: Domain-dependent patterns\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESIS 3: Domain-Dependent Scaling Behavior\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze efficiency across domains\n",
    "for domain in ['medical', 'social', 'environmental']:\n",
    "    domain_data = results_df[results_df['domain'] == domain]\n",
    "    \n",
    "    print(f\"\\n{domain.upper()} Domain:\")\n",
    "    \n",
    "    for strategy in ['Uniform', 'CritiqueMarket', 'Random']:\n",
    "        strategy_data = domain_data[domain_data['strategy_name'] == strategy]\n",
    "        print(f\"  {strategy:15s}: Efficiency = {strategy_data['efficiency'].mean():.4f} ± {strategy_data['efficiency'].std():.4f}\")\n",
    "    \n",
    "    # Test if efficiency changes with budget (scaling)\n",
    "    critique_domain = domain_data[domain_data['strategy_name'] == 'CritiqueMarket']\n",
    "    correlation = stats.pearsonr(critique_domain['budget'], critique_domain['efficiency'])\n",
    "    print(f\"  Critique scaling (budget vs efficiency): r={correlation[0]:.3f}, p={correlation[1]:.4f}\")\n",
    "\n",
    "# ANOVA: Does domain affect efficiency? (for Critique Market strategy)\n",
    "critique_only = results_df[results_df['strategy_name'] == 'CritiqueMarket']\n",
    "medical_eff = critique_only[critique_only['domain'] == 'medical']['efficiency']\n",
    "social_eff = critique_only[critique_only['domain'] == 'social']['efficiency']\n",
    "env_eff = critique_only[critique_only['domain'] == 'environmental']['efficiency']\n",
    "\n",
    "f_domain, p_domain = f_oneway(medical_eff, social_eff, env_eff)\n",
    "\n",
    "print(f\"\\n\\nANOVA: Domain effect on Critique Market efficiency\")\n",
    "print(f\"  F-statistic: {f_domain:.4f}\")\n",
    "print(f\"  p-value: {p_domain:.4f}\")\n",
    "\n",
    "if p_domain < 0.05:\n",
    "    print(f\"  ✓ Significant domain-dependent variation (p < 0.05)\")\n",
    "    print(f\"  → Scaling behavior differs across domains\")\n",
    "else:\n",
    "    print(f\"  ~ No significant domain variation (p >= 0.05)\")\n",
    "    print(f\"  → Scaling behavior similar across domains\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6771de93",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING VISUALIZATIONS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved: efficiency_by_strategy.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: scaling_curves.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: naa_comparison.png\n",
      "\n",
      "✓ All visualizations created\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create visualizations\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Figure 1: Efficiency comparison across strategies\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, domain in enumerate(['medical', 'social', 'environmental']):\n",
    "    ax = axes[idx]\n",
    "    domain_data = results_df[results_df['domain'] == domain]\n",
    "    \n",
    "    # Box plot\n",
    "    sns.boxplot(data=domain_data, x='strategy_name', y='efficiency', ax=ax)\n",
    "    ax.set_title(f'{domain.capitalize()} Domain', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Allocation Strategy', fontsize=10)\n",
    "    ax.set_ylabel('Efficiency (Findings/Cycle)', fontsize=10)\n",
    "    ax.tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/efficiency_by_strategy.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ Saved: efficiency_by_strategy.png\")\n",
    "plt.close()\n",
    "\n",
    "# Figure 2: Scaling curves (efficiency vs budget)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, domain in enumerate(['medical', 'social', 'environmental']):\n",
    "    ax = axes[idx]\n",
    "    domain_data = results_df[results_df['domain'] == domain]\n",
    "    \n",
    "    for strategy in ['Uniform', 'CritiqueMarket', 'Random']:\n",
    "        strategy_data = domain_data[domain_data['strategy_name'] == strategy]\n",
    "        strategy_data_sorted = strategy_data.sort_values('budget')\n",
    "        \n",
    "        ax.plot(strategy_data_sorted['budget'], \n",
    "                strategy_data_sorted['efficiency'], \n",
    "                marker='o', \n",
    "                label=strategy,\n",
    "                linewidth=2,\n",
    "                markersize=8)\n",
    "    \n",
    "    ax.set_title(f'{domain.capitalize()} Domain', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Budget (Cycles)', fontsize=10)\n",
    "    ax.set_ylabel('Efficiency (Findings/Cycle)', fontsize=10)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/scaling_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: scaling_curves.png\")\n",
    "plt.close()\n",
    "\n",
    "# Figure 3: NAA comparison\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "naa_data = []\n",
    "for strategy in ['Uniform', 'CritiqueMarket', 'Random']:\n",
    "    strategy_results = results_df[results_df['strategy_name'] == strategy]\n",
    "    for domain in ['medical', 'social', 'environmental']:\n",
    "        domain_results = strategy_results[strategy_results['domain'] == domain]\n",
    "        for _, row in domain_results.iterrows():\n",
    "            naa_data.append({\n",
    "                'Strategy': strategy,\n",
    "                'Domain': domain.capitalize(),\n",
    "                'NAA': row['novelty_adjusted_accuracy']\n",
    "            })\n",
    "\n",
    "naa_df = pd.DataFrame(naa_data)\n",
    "sns.barplot(data=naa_df, x='Domain', y='NAA', hue='Strategy', ax=ax)\n",
    "ax.set_title('Novelty-Adjusted Accuracy by Strategy and Domain', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Novelty-Adjusted Accuracy', fontsize=11)\n",
    "ax.set_xlabel('Domain', fontsize=11)\n",
    "ax.legend(title='Strategy', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/naa_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: naa_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n✓ All visualizations created\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba2d9c74",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "KEY FINDINGS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "1. PRIMARY HYPOTHESIS (H1): Critique Market vs. Uniform on NAA/Cycle\n",
      "   Result: NO significant difference (p=0.276)\n",
      "   Effect size: Negligible (Cohen's d=0.076)\n",
      "   Conclusion: Critique Market does NOT significantly improve novelty-adjusted\n",
      "               accuracy per cycle compared to uniform allocation\n",
      "\n",
      "2. EFFICIENCY COMPARISON (H2):\n",
      "   Uniform:  0.317 ± 0.152 findings/cycle\n",
      "   Critique: 0.320 ± 0.142 findings/cycle\n",
      "   Random:   0.194 ± 0.090 findings/cycle\n",
      "   \n",
      "   Result: ANOVA shows significant difference among strategies (p=0.038)\n",
      "   Post-hoc: Both Uniform and Critique significantly better than Random\n",
      "           But Critique vs Uniform: NO significant difference (p=0.968)\n",
      "\n",
      "3. DOMAIN-DEPENDENT SCALING (H3):\n",
      "   Result: SIGNIFICANT domain variation (p=0.026)\n",
      "   \n",
      "   Medical domain:       High efficiency (0.39), strong negative scaling\n",
      "   Social domain:        High efficiency (0.40), moderate negative scaling\n",
      "   Environmental domain: Low efficiency (0.18), strong negative scaling\n",
      "   \n",
      "   All domains show NEGATIVE scaling (efficiency decreases with budget)\n",
      "   This indicates saturation/diminishing returns\n",
      "\n",
      "4. SATURATION BEHAVIOR:\n",
      "   Medical:       r=-0.978 (p=0.022) - Strong evidence of saturation\n",
      "   Social:        r=-0.947 (p=0.053) - Moderate evidence of saturation\n",
      "   Environmental: r=-0.994 (p=0.006) - Very strong evidence of saturation\n",
      "   \n",
      "   Conclusion: Clear evidence of diminishing returns with increased budget\n",
      "               Supports H2 (saturation detected)\n",
      "\n",
      "5. OVERALL INTERPRETATION:\n",
      "   ✗ Critique Market does NOT significantly outperform Uniform allocation\n",
      "   ✓ Both structured approaches (Uniform, Critique) beat Random allocation\n",
      "   ✓ Significant domain-dependent variation observed\n",
      "   ✓ Clear saturation effects detected (diminishing returns)\n",
      "   \n",
      "   The hypothesis is PARTIALLY SUPPORTED:\n",
      "   - Part (i): Critique Market does NOT increase NAA/cycle (REJECTED)\n",
      "   - Part (ii): Domain-dependent saturation IS observed (SUPPORTED)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics and key findings\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. PRIMARY HYPOTHESIS (H1): Critique Market vs. Uniform on NAA/Cycle\")\n",
    "print(\"   Result: NO significant difference (p=0.276)\")\n",
    "print(\"   Effect size: Negligible (Cohen's d=0.076)\")\n",
    "print(\"   Conclusion: Critique Market does NOT significantly improve novelty-adjusted\")\n",
    "print(\"               accuracy per cycle compared to uniform allocation\")\n",
    "\n",
    "print(\"\\n2. EFFICIENCY COMPARISON (H2):\")\n",
    "print(\"   Uniform:  0.317 ± 0.152 findings/cycle\")\n",
    "print(\"   Critique: 0.320 ± 0.142 findings/cycle\")\n",
    "print(\"   Random:   0.194 ± 0.090 findings/cycle\")\n",
    "print(\"   \")\n",
    "print(\"   Result: ANOVA shows significant difference among strategies (p=0.038)\")\n",
    "print(\"   Post-hoc: Both Uniform and Critique significantly better than Random\")\n",
    "print(\"           But Critique vs Uniform: NO significant difference (p=0.968)\")\n",
    "\n",
    "print(\"\\n3. DOMAIN-DEPENDENT SCALING (H3):\")\n",
    "print(\"   Result: SIGNIFICANT domain variation (p=0.026)\")\n",
    "print(\"   \")\n",
    "print(\"   Medical domain:       High efficiency (0.39), strong negative scaling\")\n",
    "print(\"   Social domain:        High efficiency (0.40), moderate negative scaling\")\n",
    "print(\"   Environmental domain: Low efficiency (0.18), strong negative scaling\")\n",
    "print(\"   \")\n",
    "print(\"   All domains show NEGATIVE scaling (efficiency decreases with budget)\")\n",
    "print(\"   This indicates saturation/diminishing returns\")\n",
    "\n",
    "print(\"\\n4. SATURATION BEHAVIOR:\")\n",
    "print(\"   Medical:       r=-0.978 (p=0.022) - Strong evidence of saturation\")\n",
    "print(\"   Social:        r=-0.947 (p=0.053) - Moderate evidence of saturation\")\n",
    "print(\"   Environmental: r=-0.994 (p=0.006) - Very strong evidence of saturation\")\n",
    "print(\"   \")\n",
    "print(\"   Conclusion: Clear evidence of diminishing returns with increased budget\")\n",
    "print(\"               Supports H2 (saturation detected)\")\n",
    "\n",
    "print(\"\\n5. OVERALL INTERPRETATION:\")\n",
    "print(\"   ✗ Critique Market does NOT significantly outperform Uniform allocation\")\n",
    "print(\"   ✓ Both structured approaches (Uniform, Critique) beat Random allocation\")\n",
    "print(\"   ✓ Significant domain-dependent variation observed\")\n",
    "print(\"   ✓ Clear saturation effects detected (diminishing returns)\")\n",
    "print(\"   \")\n",
    "print(\"   The hypothesis is PARTIALLY SUPPORTED:\")\n",
    "print(\"   - Part (i): Critique Market does NOT increase NAA/cycle (REJECTED)\")\n",
    "print(\"   - Part (ii): Domain-dependent saturation IS observed (SUPPORTED)\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-11-16-22-20_CritiqueMarketsExperiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
